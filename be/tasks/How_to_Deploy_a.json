{
    "steps": [
        "Step 1: Prepare your environment\n\n* Ensure you have access to NVIDIA NGC and are able to download the Riva Quick Start resources.\n* Install NVIDIA Riva and its dependencies by following the instructions provided in the Riva Quick Start guide.\n* Ensure you have a .riva model file that you want to deploy. You can convert a .nemo model file to a .riva model file with the nemo2riva command. Alternatively, you can obtain a pre-trained Conformer-CTC .riva model for English ASR here.",
        "Step 2: Build an RMIR model pipeline from a .riva file with Riva ServiceMaker\n\n* If you encrypted your acoustic model and/or language model by adding the --key flag when invoking nemo2riva, or you downloaded a pre-trained model from NGC dated before 2023, you\u2019ll need to append a colon and then the key\u2019s value to the model\u2019s name in the riva-build command.\n* Run the following command to build the RMIR model pipeline:\n```css\nriva-build --model-path <path-to-your-riva-model-file> --rmir-path <path-to-output-rmir-file>\n```",
        "Step 3: Deploy the model locally on the Riva server\n\n* Set the path to the directory containing the Riva Quick Start resources.\n* Modify the config.sh file to enable the relevant Riva services (ASR in this case), provide the encryption key if necessary, and specify the path to the model repository (riva\\_model\\_loc) generated in the previous step.\n* Start the Riva server by running the following command:\n```arduino\nbash ${RIVA_DIR}/start_riva.sh\n```",
        "Step 4: Send inference requests from a demo client using Riva API bindings\n\n* Install the Riva Python API bindings for the client, which is available as a Python module on PyPI.\n* Call the following inference function to query the Riva server to transcribe an audio file:\n```python\nfrom riva_client import RivaClient\n\n# Initialize the Riva client\nriva_client = RivaClient(server_uri=\"localhost:50051\")\n\n# Send the inference request\nresponse = riva_client.asr(audio_file_path=\"path-to-audio-file.wav\")\n\n# Print the transcription result\nprint(response.result)\n```\nThat's it! You have successfully deployed a custom acoustic model (Conformer-CTC) trained with NVIDIA NeMo on NVIDIA Riva and can now send inference requests to transcribe audio files using the Riva server."
    ],
    "task": "How_to_Deploy_a",
    "summary_task": ""
}