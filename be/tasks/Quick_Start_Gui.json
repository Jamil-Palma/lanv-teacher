{
    "steps": [
        "Step 1:** Ensure you meet the following prerequisites:\n\n* Access and login to NVIDIA NGC.\n* Have access to an NVIDIA Volta, Turing, or Ampere architecture-based GPU.\n* Have Docker installed with support for NVIDIA GPUs.\n* Obtain a free trial license to install NVIDIA Riva from the NVIDIA AI Enterprise Trial.\n* Have at least 15 GB of free disk space on your workstation.\n\n**",
        "Step 2:** Deploy Riva Speech AI using one of the following options:\n\n* **Local Docker:**\n\t1. Download the Quick Start scripts for your platform (Linux x86_64 or Linux ARM64) from the NVIDIA NGC catalog or using the NGC CLI tool.\n\t2. Initialize and start Riva by running the `riva_init.sh` script. This will download and prepare Docker images and models.\n\t3. Launch the server by running the `riva_start.sh` script. This can take up to an hour on an average internet connection.\n\t4. Optionally, modify the `config.sh` file within the quickstart directory with your preferred configuration.\n* **Kubernetes (not supported for embedded platforms):**\n\t1. Follow the instructions in the NVIDIA Riva documentation to deploy Riva to a Kubernetes cluster using the Riva Helm Chart.\n\n**",
        "Step 3:** Ensure that you have modified the `config.sh` file within the quickstart directory with your preferred configuration, as necessary.\n\n**",
        "Step 4:** Initialize and start Riva by running the appropriate script based on your deployment option.\n\n**",
        "Step 5:** Run through the different tutorials on GitHub to use the Riva API.\n\n**",
        "Step 6:** Use Riva command-line clients to perform various tasks, such as:\n\n* Transcribe audio (Automatic Speech Recognition or ASR) by running the `riva_asr.sh` script for streaming or offline recognition.\n* Synthesize text-to-speech (TTS) by running the `riva_tts.sh` script.\n* Translate text or speech using NeMo (NVIDIA's open-source toolkit for conversational AI) by running the appropriate commands.\n\n**",
        "Step 7:** Shut down the server when finished.\n\n**",
        "Step 8:** For more information on how to customize a local deployment, refer to the NVIDIA Riva documentation on Local (Docker).\n\nBy following these steps, you will be able to deploy the Riva server with pretrained models and use the API for various speech AI tasks. To learn more about Riva Speech AI Skills and how they can be used in real applications, visit the NVIDIA Riva Developer page."
    ],
    "task": "Quick_Start_Gui",
    "summary_task": " The NVIDIA Riva Quick Start Guide is a comprehensive document that helps users deploy pretrained models for speech AI skills on a local workstation. The guide supports two architectures: Linux x86\\_64 (data center) and Linux ARM64 (embedded), with a focus on the embedded version, currently in public beta. To use Riva Speech AI, users must have access to a compatible NVIDIA GPU, Docker installed with support for NVIDIA GPUs, and a free trial license from NVIDIA AI Enterprise. The guide provides instructions for deploying Riva Speech AI using pretrained models available from the NGC catalog, with two options: Local Docker and Kubernetes, the latter not supported for embedded. Users can also run Riva Speech AI with fine-tuned custom models using NVIDIA NeMo. The guide includes Quick Start scripts for deploying Riva Speech AI services locally, covering data center and embedded platforms. Users can customize their deployment by modifying the config.sh file. The guide provides detailed steps for using Riva Speech AI, such as configuring translation services, executing command-line clients, and testing speech-to-text, text-to-speech, and text/speech translation. Additional resources and information on building speech AI applications with Riva APIs are also provided."
}